{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bound-natural",
   "metadata": {
    "id": "placed-radical"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession, Row, functions\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "chubby-evaluation",
   "metadata": {
    "id": "reflected-liquid"
   },
   "outputs": [],
   "source": [
    "# initialize SparkSession\n",
    "sc=SparkContext()\n",
    "spark=SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "insured-authority",
   "metadata": {
    "id": "current-lesson"
   },
   "outputs": [],
   "source": [
    "# Create DataFrames\n",
    "path = os.getcwd()\n",
    "df_M = spark.read.format(\"csv\").load(path + \"/data/movies.csv\")\n",
    "df_R = spark.read.format(\"csv\").load(path + \"/data/ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "roman-tourism",
   "metadata": {
    "id": "cellular-updating"
   },
   "outputs": [],
   "source": [
    "#rename columns\n",
    "df2= df_R.withColumnRenamed(\"_c0\", \"userId\").withColumnRenamed(\"_c1\", \"movieId\").withColumnRenamed(\"_c2\", \"rating\").withColumnRenamed(\"_c3\",\"timestamp\")\n",
    "df1= df_M.withColumnRenamed(\"_c0\", \"movieId\").withColumnRenamed(\"_c1\", \"title\").withColumnRenamed(\"_c2\", \"genre\")\n",
    "\n",
    "# Join DataFrames\n",
    "df1=df1.join(df2,on=\"movieId\")\n",
    "\n",
    "# Create Train/Test split\n",
    "split = df1.randomSplit([0.8,0.2],seed=1)\n",
    "train = split[0]\n",
    "test = split[1]\n",
    "\n",
    "# casting\n",
    "train=train.withColumn(\"userId\", train[\"userId\"].cast(\"int\"))\\\n",
    "           .withColumn(\"movieId\", train[\"movieId\"].cast(\"int\"))\\\n",
    "           .withColumn(\"rating\", train[\"rating\"].cast(\"float\"))\n",
    "\n",
    "test=test.withColumn(\"userId\", test[\"userId\"].cast(\"int\"))\\\n",
    "         .withColumn(\"movieId\", test[\"movieId\"].cast(\"int\"))\\\n",
    "         .withColumn(\"rating\", test[\"rating\"].cast(\"float\"))\n",
    "\n",
    "# remove columns where movieId, userId, or rating is null\n",
    "train=train.filter(train.movieId.isNotNull())\n",
    "train=train.filter(train.userId.isNotNull())\n",
    "train=train.filter(train.rating.isNotNull())\n",
    "test=test.filter(test.movieId.isNotNull())\n",
    "test=test.filter(test.userId.isNotNull())\n",
    "test=test.filter(test.rating.isNotNull())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "outdoor-fancy",
   "metadata": {
    "id": "acting-annual",
    "outputId": "dbbb20de-9970-4eca-a39c-15cbf978f207"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.51 s ± 1.01 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# Time the process\n",
    "\n",
    "#compute biases for movies and users and the average movie rating using the trainset\n",
    "\n",
    "# first compute overall average rating\n",
    "\n",
    "#select only column \"rating\"\n",
    "#transform to rdd\n",
    "#each line in the rdd will be of type Row. Transform Row to a dicitonary and then access value of \"rating\" to convert it to float\n",
    "ratings = train.select(\"rating\")\\\n",
    "               .rdd\\\n",
    "               .map(lambda x: float(x.asDict()[\"rating\"]))  \n",
    "               \n",
    "#sum all ratings in the rdd\n",
    "sum_ratings = ratings.sum()\n",
    "#total number of ratings in the rdd\n",
    "total_ratings = ratings.count()\n",
    "\n",
    "avg_rating= sum_ratings/total_ratings\n",
    "\n",
    "#compute user biases and movie biases\n",
    "\n",
    "#compute number of ratings for each user\n",
    "train1 = train.groupby(\"userId\")\\\n",
    "              .agg({\"rating\":\"count\"})\\\n",
    "              .withColumnRenamed(\"count(rating)\", \"count_Rating\")\n",
    "\n",
    "#compute sum of ratings for each user     \n",
    "train2= train.groupby(\"userId\")\\\n",
    "             .agg({\"rating\":\"sum\"})\\\n",
    "             .withColumnRenamed(\"sum(rating)\", \"sum_Rating\")\n",
    "\n",
    "#compute average user rating for each user\n",
    "train2=train2.join(train1, on=\"userId\")\n",
    "train2=train2.withColumn(\"avg_User_Rating\", train2[\"sum_Rating\"]/train2[\"count_Rating\"])\n",
    "\n",
    "\n",
    "\n",
    "#compute User biases\n",
    "\n",
    "rdd = train2.rdd.map(lambda x:funct(x))\n",
    "\n",
    "def funct(x):\n",
    "    bias_User=x[\"avg_User_Rating\"]-avg_rating\n",
    "    r_1=x[\"userId\"]\n",
    "    return Row(userId=r_1, bias_User=bias_User)\n",
    "    \n",
    "\n",
    "#train_rdd.take(20)\n",
    "user_biases= spark.createDataFrame(rdd)\n",
    "\n",
    "#user_biases.sort(\"userId\").show(n=20)\n",
    "\n",
    "#compute movie biases\n",
    "\n",
    "\n",
    "\n",
    "#compute number of ratings for each movie\n",
    "train1 = train.groupby(\"movieId\")\\\n",
    "              .agg({\"rating\":\"count\"})\\\n",
    "              .withColumnRenamed(\"count(rating)\", \"count_Rating\")\n",
    "\n",
    "#compute sum of ratings for each movie     \n",
    "train2= train.groupby(\"movieId\")\\\n",
    "             .agg({\"rating\":\"sum\"})\\\n",
    "             .withColumnRenamed(\"sum(rating)\", \"sum_Rating\")\n",
    "\n",
    "\n",
    "\n",
    "#compute average movie rating for each movie\n",
    "train2=train2.join(train1, on=\"movieId\")\n",
    "train2=train2.withColumn(\"avg_Movie_Rating\", train2[\"sum_Rating\"]/train2[\"count_Rating\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#compute Movie biases\n",
    "\n",
    "rdd = train2.rdd.map(lambda x:funct(x))\n",
    "\n",
    "def funct(x):\n",
    "    bias_Movie=x[\"avg_Movie_Rating\"]-avg_rating\n",
    "    r_1=x[\"movieId\"]\n",
    "    return Row(movieId=r_1,bias_Movie=bias_Movie)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "movie_biases= spark.createDataFrame(rdd)\n",
    "\n",
    "#movie_biases.sort(\"movieId\").show(n=20)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "atomic-detroit",
   "metadata": {
    "id": "electric-superintendent"
   },
   "outputs": [],
   "source": [
    "# Now actually compute biases for movies and users and the average movie rating using the trainset\n",
    "\n",
    "# first compute overall average rating\n",
    "\n",
    "#select only column \"rating\"\n",
    "#transform to rdd\n",
    "#each line in the rdd will be of type Row. Transform Row to a dicitonary and then access value of \"rating\" to convert it to float\n",
    "ratings = train.select(\"rating\")\\\n",
    "               .rdd\\\n",
    "               .map(lambda x: float(x.asDict()[\"rating\"]))  \n",
    "               \n",
    "#sum all ratings in the rdd\n",
    "sum_ratings = ratings.sum()\n",
    "#total number of ratings in the rdd\n",
    "total_ratings = ratings.count()\n",
    "\n",
    "avg_rating= sum_ratings/total_ratings\n",
    "\n",
    "#compute user biases and movie biases\n",
    "\n",
    "#compute number of ratings for each user\n",
    "train1 = train.groupby(\"userId\")\\\n",
    "              .agg({\"rating\":\"count\"})\\\n",
    "              .withColumnRenamed(\"count(rating)\", \"count_Rating\")\n",
    "\n",
    "#compute sum of ratings for each user     \n",
    "train2= train.groupby(\"userId\")\\\n",
    "             .agg({\"rating\":\"sum\"})\\\n",
    "             .withColumnRenamed(\"sum(rating)\", \"sum_Rating\")\n",
    "\n",
    "#compute average user rating for each user\n",
    "train2=train2.join(train1, on=\"userId\")\n",
    "train2=train2.withColumn(\"avg_User_Rating\", train2[\"sum_Rating\"]/train2[\"count_Rating\"])\n",
    "\n",
    "\n",
    "\n",
    "#compute User biases\n",
    "\n",
    "rdd = train2.rdd.map(lambda x:funct(x))\n",
    "\n",
    "def funct(x):\n",
    "    bias_User=x[\"avg_User_Rating\"]-avg_rating\n",
    "    r_1=x[\"userId\"]\n",
    "    return Row(userId=r_1, bias_User=bias_User)\n",
    "    \n",
    "\n",
    "#train_rdd.take(20)\n",
    "user_biases= spark.createDataFrame(rdd)\n",
    "\n",
    "#user_biases.sort(\"userId\").show(n=20)\n",
    "\n",
    "#compute movie biases\n",
    "\n",
    "\n",
    "\n",
    "#compute number of ratings for each movie\n",
    "train1 = train.groupby(\"movieId\")\\\n",
    "              .agg({\"rating\":\"count\"})\\\n",
    "              .withColumnRenamed(\"count(rating)\", \"count_Rating\")\n",
    "\n",
    "#compute sum of ratings for each movie     \n",
    "train2= train.groupby(\"movieId\")\\\n",
    "             .agg({\"rating\":\"sum\"})\\\n",
    "             .withColumnRenamed(\"sum(rating)\", \"sum_Rating\")\n",
    "\n",
    "\n",
    "\n",
    "#compute average movie rating for each movie\n",
    "train2=train2.join(train1, on=\"movieId\")\n",
    "train2=train2.withColumn(\"avg_Movie_Rating\", train2[\"sum_Rating\"]/train2[\"count_Rating\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#compute Movie biases\n",
    "\n",
    "rdd = train2.rdd.map(lambda x:funct(x))\n",
    "\n",
    "def funct(x):\n",
    "    bias_Movie=x[\"avg_Movie_Rating\"]-avg_rating\n",
    "    r_1=x[\"movieId\"]\n",
    "    return Row(movieId=r_1,bias_Movie=bias_Movie)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "movie_biases= spark.createDataFrame(rdd)\n",
    "\n",
    "#movie_biases.sort(\"movieId\").show(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "mineral-fancy",
   "metadata": {
    "id": "processed-citizen",
    "outputId": "30b58aed-a963-4370-a3d0-0c4aa01267b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206 ms ± 23.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "#predict user ratings for movies and compare these ratings with the ratings included in the testset\n",
    "\n",
    "#join test set with user_biases and movie_biases\n",
    "testing= test.join(user_biases, on=\"userId\")\\\n",
    "          .join(movie_biases, on=\"movieId\")\n",
    "\n",
    "testing=testing.rdd.map(lambda x: predict_funct(x))\n",
    "\n",
    "def predict_funct(x):\n",
    "    pred=x[\"bias_User\"]+x[\"bias_Movie\"]+avg_rating\n",
    "    r_1=x[\"movieId\"]\n",
    "    r_2=x[\"userId\"]\n",
    "    r_3=x[\"title\"]\n",
    "    r_4=x[\"rating\"]\n",
    "    \n",
    "    return Row(movieId=r_1, userId=r_2, title=r_3, rating=r_4, prediction=pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acceptable-soundtrack",
   "metadata": {
    "id": "acquired-standing"
   },
   "outputs": [],
   "source": [
    "# Now actually predict user ratings for movies and compare these ratings with the ratings included in the testset\n",
    "\n",
    "#join test set with user_biases and movie_biases\n",
    "testing= test.join(user_biases, on=\"userId\")\\\n",
    "          .join(movie_biases, on=\"movieId\")\n",
    "\n",
    "testing=testing.rdd.map(lambda x: predict_funct(x))\n",
    "\n",
    "def predict_funct(x):\n",
    "    pred=x[\"bias_User\"]+x[\"bias_Movie\"]+avg_rating\n",
    "    r_1=x[\"movieId\"]\n",
    "    r_2=x[\"userId\"]\n",
    "    r_3=x[\"title\"]\n",
    "    r_4=x[\"rating\"]\n",
    "    \n",
    "    return Row(movieId=r_1, userId=r_2, title=r_3, rating=r_4, prediction=pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "civilian-gnome",
   "metadata": {
    "id": "assumed-williams",
    "outputId": "f3b98286-e6b8-4d51-9a64-2692315fefd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 0.9057333522741469\n"
     ]
    }
   ],
   "source": [
    "# compute the error of our predictions\n",
    "\n",
    "testing= spark.createDataFrame(testing)\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(testing)\n",
    "print(\"Root-mean-square error = \" + str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "collaborative-export",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+--------------------+\n",
      "|userId|   max(prediction)|        first(title)|\n",
      "+------+------------------+--------------------+\n",
      "|    26| 4.002829291433965| Pulp Fiction (1994)|\n",
      "|    29| 5.019706212895851|    The Alamo (2004)|\n",
      "|   474| 4.892961603463069|Brief Encounter (...|\n",
      "|    65| 4.963587808294701|      Memento (2000)|\n",
      "|   191| 4.383381677216477|Wallace & Gromit:...|\n",
      "|   418| 4.285603721604014|Harry Potter and ...|\n",
      "|   541|4.0307919028205355|      Aladdin (1992)|\n",
      "|   558| 4.634142422747097| Pulp Fiction (1994)|\n",
      "|   222| 4.623847309145523|Léon: The Profess...|\n",
      "|   270|3.6058032158699986|Willy Wonka & the...|\n",
      "|   293| 2.795405137627034|Beverly Hills Cop...|\n",
      "|   243|  5.01745226818362|      Aladdin (1992)|\n",
      "|   278| 4.492472586307386|Muppet Movie, The...|\n",
      "|   367| 4.721353114181202|  Skulls, The (2000)|\n",
      "|   442| 2.032085914156008|          JFK (1991)|\n",
      "|    19| 3.423281185248839|In the Line of Fi...|\n",
      "|    54|3.7667181803228544| Pulp Fiction (1994)|\n",
      "|   296|4.8730414326657066|In the Mood For L...|\n",
      "|   277|3.2819642774119804|      Twister (1996)|\n",
      "|   287|3.3303513741861734|City of Lost Chil...|\n",
      "+------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate top movie recommendations for users\n",
    "testing = testing.groupby(\"userId\").agg(functions.max(\"prediction\"),functions.first(\"title\"))\n",
    "# Show results for 20 users\n",
    "testing.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-winner",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "homework3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
