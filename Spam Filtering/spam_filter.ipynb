{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "minute-burden",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.mllib.feature import HashingTF, IDF\n",
    "from pyspark.ml.feature import CountVectorizer, IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "persistent-september",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=SparkContext()\n",
    "\n",
    "spark=SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "electronic-soundtrack",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "rdd_nospam_train=sc.textFile(path + \"/data/nospam_training.txt\")\n",
    "rdd_spam_train=sc.textFile(path + \"/data/spam_training.txt\")\n",
    "rdd_nospam_test=sc.textFile(path + \"/data/nospam_testing.txt\")\n",
    "rdd_spam_test=sc.textFile(path + \"/data/spam_testing.txt\")\n",
    "\n",
    "\n",
    "\n",
    "#for spam and no spam datasets: split each line of the text by\n",
    "#looking for any number of whitespaces between actual words\n",
    "#also add a label for each sample of the datasets\n",
    "rdd_nospam_train=rdd_nospam_train.map(lambda x: Row(label=0, text=x.split()))\n",
    "df_nospam=spark.createDataFrame(rdd_nospam_train)\n",
    "\n",
    "rdd_spam_train=rdd_spam_train.map(lambda x: Row(label=1, text=x.split()))\n",
    "df_spam=spark.createDataFrame(rdd_spam_train)\n",
    "\n",
    "rdd_nospam_test=rdd_nospam_test.map(lambda x: Row(label=0, text=x.split()))\n",
    "df_test_nospam=spark.createDataFrame(rdd_nospam_test)\n",
    "\n",
    "rdd_spam_test=rdd_spam_test.map(lambda x: Row(label=1, text=x.split()))\n",
    "df_test_spam=spark.createDataFrame(rdd_spam_test)\n",
    "\n",
    "#perform a union of the two datsets\n",
    "df= df_nospam.union(df_spam)\n",
    "df_test= df_test_nospam.union(df_test_spam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "separate-access",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we create our CountVectorizerModel with the training data\n",
    "cv = CountVectorizer(inputCol = \"text\", outputCol = \"features\", vocabSize = 1000)\n",
    "model =cv.fit(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "false-spare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we apply the CountVectorizerModel to our training DataFrame, so that for every Row we get the term frequency of relevant words\n",
    "result= model.transform(df)\n",
    "result.collect()\n",
    "# we apply the same CountVectorizerModel obtained through the training set, to our test DataFrame\n",
    "result_test=model.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sustainable-runner",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create the InverseDoucmentFrequency Model with our training set\n",
    "idf = IDF(inputCol=\"features\", outputCol = \"idf_features\")\n",
    "idfModel = idf.fit(result)\n",
    "\n",
    "#we apply the InverseDocumentFrequency Model to our trainingset\n",
    "r= idfModel.transform(result)\n",
    "\n",
    "#we apply the InverseDocumentFrequency Model to our test set\n",
    "r_test= idfModel.transform(result_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "harmful-ambassador",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol=\"idf_features\")\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "patient-florence",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_test1=lrModel.transform(r_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "italic-newton",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_r_test1=r_test1.rdd.map(lambda x: (x[\"prediction\"], float(x[\"label\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "christian-empire",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the error of our predictions\n",
    "\n",
    "text_file = open(\"spamfilter_evaluation.txt\", \"w\")\n",
    "text_file.write(\"Confusion Matrix:\\n\")\n",
    "text_file.write(str(MulticlassMetrics(rdd_r_test1).confusionMatrix().toArray()[:,0])+\"\\n\")\n",
    "text_file.write(str(MulticlassMetrics(rdd_r_test1).confusionMatrix().toArray()[:,1])+\"\\n\")\n",
    "\n",
    "text_file.write(\"The rows are true labels and the columns predictions. (First Row: Spam, Second Row: No-Spam. Columns ordered analogous)\\n\")\n",
    "text_file.write(\"\\n\")\n",
    "\n",
    "text_file.write(\"Recall Spam:\" + str(MulticlassMetrics(rdd_r_test1).recall(label=1)) +\"\\n\")\n",
    "text_file.write(\"Recall No-Spam:\" + str(MulticlassMetrics(rdd_r_test1).recall(label=0))+\"\\n\")\n",
    "text_file.write(\"Precision Spam:\" + str(MulticlassMetrics(rdd_r_test1).precision(label=1))+\"\\n\")\n",
    "text_file.write(\"Precision No-Spam:\" + str(MulticlassMetrics(rdd_r_test1).precision(label=0)))\n",
    "\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "defined-relationship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[924.  41.]\n",
      " [ 24. 125.]]\n",
      "The rows are true labels and the columns predictions. (First Row: Spam, Second Row: No-Spam. Columns ordered analogous)\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix:\")\n",
    "print(MulticlassMetrics(rdd_r_test1).confusionMatrix().toArray())\n",
    "print(\"The rows are true labels and the columns predictions. (First Row: Spam, Second Row: No-Spam. Columns ordered analogous)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "critical-effectiveness",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Spam:0.8389261744966443\n",
      "Recall No-Spam:0.9575129533678757\n",
      "Precision Spam:0.7530120481927711\n",
      "Precision No-Spam:0.9746835443037974\n"
     ]
    }
   ],
   "source": [
    "print(\"Recall Spam:\" + str(MulticlassMetrics(rdd_r_test1).recall(label=1)))\n",
    "print(\"Recall No-Spam:\" + str(MulticlassMetrics(rdd_r_test1).recall(label=0)))\n",
    "\n",
    "print(\"Precision Spam:\" + str(MulticlassMetrics(rdd_r_test1).precision(label=1)))\n",
    "print(\"Precision No-Spam:\" + str(MulticlassMetrics(rdd_r_test1).precision(label=0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-center",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
